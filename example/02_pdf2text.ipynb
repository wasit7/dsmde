{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd64c54-4f80-4f87-af48-1862f99a4b0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Course 2: Document File Conversion to Text for Data Analysts\n",
    "\n",
    "## Hypothetical Scenario:\n",
    "Imagine you are a data analyst working with a team that's tasked to digitize decades of government documents from the Thai Ministry of Education. These documents are only available in PDF format, and they contain vital statistics about school performance that need to be analyzed and reported. The challenge is to convert thousands of pages of PDF documents into a text format that can be ingested into a database for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b0add-6db3-496c-8d07-b7787e9fe80b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 1: Overview of Document Conversion Challenges\n",
    "**Content**: Document conversion is a necessary step in data analysis when dealing with various sources of data. Challenges include maintaining the integrity of the data, dealing with different file formats, handling encoding issues, and automating the conversion process for large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a254379-6829-478d-bf13-c8a2ab232e9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Section 2: Introduction to PDF-to-Text Conversion Techniques\n",
    "**Content**: PDF-to-text conversion can be done through several methods including using OCR (Optical Character Recognition) for scanned documents or extracting text from digital PDFs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378aafb3-d369-4379-ab05-26faa0607bed",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Section 3: Working with Python PDF Libraries (e.g., PyPDF2)\n",
    "**Content**: PyPDF2 is a library in Python that allows you to perform many operations on PDFs, including extracting text.\n",
    "\n",
    "```python\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "# Open the PDF file\n",
    "with open('example.pdf', 'rb') as file:\n",
    "    reader = PdfFileReader(file)\n",
    "    page = reader.getPage(0)\n",
    "    text = page.extractText()\n",
    "    print(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3dab11-7df9-420f-ac46-09fa926e1280",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 4: Regular Expressions for Text Cleaning\n",
    "**Content**: Regular expressions are used to identify patterns in text, which is helpful for cleaning and organizing data extracted from PDFs.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"Sample text with some numbers 1234 and symbols #%&!\"\n",
    "cleaned_text = re.sub(r'[^A-Za-z0-9ก-๙\\s]', '', text)  # Assuming Thai and English text\n",
    "print(cleaned_text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52100aae-f226-4b40-8269-62cd86b6e968",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 5: Addressing Encoding Issues in Thai Articles\n",
    "**Content**: Thai language characters may not be correctly encoded, leading to gibberish text. Proper encoding handling is essential.\n",
    "\n",
    "```python\n",
    "original_text = b'\\xca\\xfe'  # Example bytes that represent Thai characters incorrectly encoded\n",
    "correct_encoding = original_text.decode('tis-620')  # Decoding with the correct encoding for Thai\n",
    "print(correct_encoding)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86b6d7-718e-4a6b-a423-63778a71836a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 6: Introduction to the RU02 API Service\n",
    "**Content**: RU02 API service may provide an API for converting PDF documents into text. It handles different encoding and document layouts internally.\n",
    "\n",
    "```python\n",
    "# Pseudocode for API interaction\n",
    "import requests\n",
    "\n",
    "response = requests.post('https://ru02api.service/convert', files={'file': open('example.pdf', 'rb')})\n",
    "text = response.text\n",
    "print(text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b060d41-8ba4-49b3-8421-773447c6ee47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 7: Automating PDF to Text Conversion\n",
    "**Content**: Automation of PDF conversion can be done using scripts that interact with conversion libraries or APIs.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "# Directory with PDF files\n",
    "pdf_dir = '/path/to/pdf_files/'\n",
    "text_dir = '/path/to/text_output/'\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        with open(os.path.join(pdf_dir, pdf_file), 'rb') as file:\n",
    "            reader = PdfFileReader(file)\n",
    "            text_content = []\n",
    "            for page_num in range(reader.numPages):\n",
    "                text_content.append(reader.getPage(page_num).extractText())\n",
    "            \n",
    "            with open(os.path.join(text_dir, pdf_file + '.txt'), 'w') as text_file:\n",
    "                text_file.write('\\n'.join(text_content))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208ea2b-2f13-45ab-ae02-2be7f7042ba5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 8: Quality Assurance for Converted Texts\n",
    "**Content**: Quality assurance involves checking the converted text against the original document to ensure accuracy.\n",
    "\n",
    "```python\n",
    "# Pseudocode for quality check\n",
    "def quality_check(original_pdf, converted_text):\n",
    "    # Implement comparison logic here\n",
    "    pass\n",
    "\n",
    "# Use this function to compare PDF and text\n",
    "quality_check('original.pdf', 'converted_text.txt')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c404c50-d8c9-492e-aade-e6588e156ff8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 9: Real-World Application: Extracting Data from Academic Papers\n",
    "**Content**: Data extraction from academic papers requires recognizing the structure of the document to extract elements like titles, authors, and content.\n",
    "\n",
    "```python\n",
    "# Pseudocode for structured extraction\n",
    "def extract_academic_data(text):\n",
    "    # Implement logic to identify and extract different sections\n",
    "    pass\n",
    "\n",
    "# Use this function to process extracted text\n",
    "extract_academic_data(converted_text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c91a40-4f3f-41e4-b198-5c032f76f13b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 10: Advanced Text Cleaning and Preprocessing Techniques\n",
    "**Content**: Advanced techniques involve using natural language processing to further clean and preprocess the text.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import\n",
    "\n",
    " word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"An example sentence that needs to be cleaned.\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27a8ce-66a9-4fc2-8faa-3baf8206981d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 11: Best Practices for Document Conversion Workflows\n",
    "**Content**: Discussing best practices such as document version control, testing conversion processes, and regular updates to conversion scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df851ac-ec38-4b29-ab18-cc082d214e63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 12: Case Study: Legal Document Analysis\n",
    "**Content**: Legal documents often have a specific jargon and format. Analysis of these documents requires an understanding of legal terminology and the structure of legal texts.\n",
    "\n",
    "```python\n",
    "# Pseudocode for legal terminology extraction\n",
    "def extract_legal_terms(text):\n",
    "    # Implement logic for identifying legal terms\n",
    "    pass\n",
    "\n",
    "# Use this function to process legal documents\n",
    "extract_legal_terms(converted_text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260a250-11f4-4e99-93d1-603c97f94688",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 13: Course Project: Building a Document Conversion Pipeline\n",
    "**Content**: For the course project, students will build a complete pipeline to convert PDFs into text, clean the text, and prepare it for analysis.\n",
    "\n",
    "```python\n",
    "# Pseudocode for pipeline\n",
    "def document_conversion_pipeline(pdf_file_path):\n",
    "    # Implement the pipeline steps here\n",
    "    pass\n",
    "\n",
    "# Execute the pipeline for a document\n",
    "document_conversion_pipeline('example.pdf')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9954d-1622-4be9-9bf2-077a5acedd5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 14: Course Review and Additional Resources\n",
    "**Content**: Review of the key topics covered in the course and discussion of additional resources for further learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c46e5-ed4e-4ef2-b498-7b757fe51639",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Section 15: Final Assessment and Next Steps\n",
    "**Content**: The final assessment might involve converting a set of PDFs, ensuring quality, and demonstrating understanding of the process. Next steps include exploring further applications of document conversion in data analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
