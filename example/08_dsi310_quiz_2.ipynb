{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec90c40-79d4-4655-930f-73f4161fa585",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "File management and web scraping in Python, using real-world websites and scenarios. Each question will be designed to test and enhance different aspects of these skills. \n",
    "\n",
    "* Quiz 1: Basic File Reading and Writing\n",
    "* Quiz 2: Web Scraping Basic HTML Data\n",
    "* Quiz 3: Web Scraping with Pagination\n",
    "* Quiz 4: Advanced File Operations\n",
    "* Quiz 5: Web Scraping Dynamic Content\n",
    "* Quiz 6: Extracting and Analyzing Data from API\n",
    "* Quiz 7: Scraping and Processing E-commerce Product Data\n",
    "* Quiz 8: Automated Data Cleaning from a Text File\n",
    "* Quiz 9: Parsing and Summarizing Data from a News API\n",
    "* Quiz 10: Web Scraping with JavaScript-Rendered Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a5dc1-7d34-465d-8c9a-4c3e5d75b237",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 1: Basic File Reading and Writing\n",
    "**Task**: Write a Python script to read a CSV file containing movie data from [IMDb](https://www.imdb.com/interfaces/), then convert and save this data into a JSON file. The script should be able to handle basic data cleaning like trimming whitespace from strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c68a4dd-2840-4435-b7d2-d3bcb949c8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10345990 entries, 0 to 10345989\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   Unnamed: 0      int64 \n",
      " 1   tconst          object\n",
      " 2   titleType       object\n",
      " 3   primaryTitle    object\n",
      " 4   originalTitle   object\n",
      " 5   isAdult         object\n",
      " 6   startYear       object\n",
      " 7   endYear         object\n",
      " 8   runtimeMinutes  object\n",
      " 9   genres          object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 789.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tconst</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tt0000001</td>\n",
       "      <td>short</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0000002</td>\n",
       "      <td>short</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>5</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0000003</td>\n",
       "      <td>short</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>4</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0000004</td>\n",
       "      <td>short</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>12</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0000005</td>\n",
       "      <td>short</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>0</td>\n",
       "      <td>1893</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Comedy,Short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     tconst titleType            primaryTitle  \\\n",
       "0           0  tt0000001     short              Carmencita   \n",
       "1           1  tt0000002     short  Le clown et ses chiens   \n",
       "2           2  tt0000003     short          Pauvre Pierrot   \n",
       "3           3  tt0000004     short             Un bon bock   \n",
       "4           4  tt0000005     short        Blacksmith Scene   \n",
       "\n",
       "            originalTitle isAdult startYear endYear runtimeMinutes  \\\n",
       "0              Carmencita       0      1894      \\N              1   \n",
       "1  Le clown et ses chiens       0      1892      \\N              5   \n",
       "2          Pauvre Pierrot       0      1892      \\N              4   \n",
       "3             Un bon bock       0      1892      \\N             12   \n",
       "4        Blacksmith Scene       0      1893      \\N              1   \n",
       "\n",
       "                     genres  \n",
       "0         Documentary,Short  \n",
       "1           Animation,Short  \n",
       "2  Animation,Comedy,Romance  \n",
       "3           Animation,Short  \n",
       "4              Comedy,Short  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quiz_01\n",
    "#data source: https://developer.imdb.com/non-commercial-datasets/\n",
    "import pandas as pd\n",
    "# file_path = 'https://datasets.imdbws.com/title.basics.tsv.gz'\n",
    "file_path = './dataset/title.basics.tsv.gz'\n",
    "df=pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab0ab1-94c2-4da6-a5b5-609b8f45f0fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 2: Web Scraping Basic HTML Data\n",
    "**Task**: Write a Python script using `BeautifulSoup` to scrape the current top news headlines from [BBC News](https://www.bbc.com/news). Extract the headline text and the corresponding URLs, and save them in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a932ef-48d0-4e2e-8eab-443226f3e0d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39 entries, 0 to 38\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   headline  39 non-null     object\n",
      " 1   url       39 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 752.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>More hostages released despite agonising delay</td>\n",
       "      <td>https://www.bbc.com/news/world-middle-east-675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'I thought he died' - joy after Thai hostage f...</td>\n",
       "      <td>https://www.bbc.com/news/world-middle-east-675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India rescuers to dig by hand after drill breaks</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-india-6753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crowds cheer Palestinians released from Israel...</td>\n",
       "      <td>https://www.bbc.com/news/world-middle-east-675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indo-Chinese cuisine makes a splash in US dining</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-india-6741...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0     More hostages released despite agonising delay   \n",
       "1  'I thought he died' - joy after Thai hostage f...   \n",
       "2   India rescuers to dig by hand after drill breaks   \n",
       "3  Crowds cheer Palestinians released from Israel...   \n",
       "4   Indo-Chinese cuisine makes a splash in US dining   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.bbc.com/news/world-middle-east-675...  \n",
       "1  https://www.bbc.com/news/world-middle-east-675...  \n",
       "2  https://www.bbc.com/news/world-asia-india-6753...  \n",
       "3  https://www.bbc.com/news/world-middle-east-675...  \n",
       "4  https://www.bbc.com/news/world-asia-india-6741...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching the webpage\n",
    "response = requests.get(\"https://www.bbc.com/news\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extracting headlines and URLs\n",
    "articles = soup.find_all('h3')\n",
    "data = [{'headline': article.get_text(strip=True), 'url': 'https://www.bbc.com' + article.find_parent('a')['href']} for article in articles if article.find_parent('a')]\n",
    "\n",
    "# Saving to CSV\n",
    "pd.DataFrame(data).to_csv('bbc_news_headlines.csv', index=False)\n",
    "df = pd.read_csv('./dataset/bbc_news_headlines.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244f822-7ee7-4809-ad41-9b32e6719744",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 3: Web Scraping with Pagination\n",
    "**Task**: Create a Python script to scrape job listings from the first three pages of [Indeed](https://www.indeed.com) for a specific job title and location. The script should extract the job title, company name, location, and summary of each listing and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "295e31c9-e5aa-4bf6-a208-df48431b66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Job Title  30 non-null     object\n",
      " 1   Company    30 non-null     object\n",
      " 2   Location   30 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 848.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test Software Development Engineer</td>\n",
       "      <td>Lumentum Operations LLC</td>\n",
       "      <td>นิคมอุตสาหกรรมนวนคร</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engineer, Mfg Process Sustaining</td>\n",
       "      <td>Lumentum Operations LLC</td>\n",
       "      <td>นิคมอุตสาหกรรมนวนคร</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analysis Engineer</td>\n",
       "      <td>Kubota Research &amp; Development Asia</td>\n",
       "      <td>ปทุมธานี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Structural Engineer, AITS</td>\n",
       "      <td>Asian Institute of Technology</td>\n",
       "      <td>คลองหลวง, ปทุมธานี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Process Engineer</td>\n",
       "      <td>Lumentum Operations LLC</td>\n",
       "      <td>นิคมอุตสาหกรรมนวนคร</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Job Title                             Company  \\\n",
       "0  Test Software Development Engineer             Lumentum Operations LLC   \n",
       "1    Engineer, Mfg Process Sustaining             Lumentum Operations LLC   \n",
       "2                   Analysis Engineer  Kubota Research & Development Asia   \n",
       "3           Structural Engineer, AITS       Asian Institute of Technology   \n",
       "4                    Process Engineer             Lumentum Operations LLC   \n",
       "\n",
       "              Location  \n",
       "0  นิคมอุตสาหกรรมนวนคร  \n",
       "1  นิคมอุตสาหกรรมนวนคร  \n",
       "2             ปทุมธานี  \n",
       "3   คลองหลวง, ปทุมธานี  \n",
       "4  นิคมอุตสาหกรรมนวนคร  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "async def scrape_indeed_jobs(job_title, location):\n",
    "    job_listings = []\n",
    "    pw = await async_playwright().start()\n",
    "    browser = await pw.chromium.launch(headless = False)\n",
    "    page = await browser.new_page()\n",
    "    for start in range(0, 30, 15):  # 0, 10, 20 for the first three pages\n",
    "        params = {\n",
    "            'q': job_title,\n",
    "            'l': location,\n",
    "            'start': start\n",
    "        }\n",
    "        url=f'https://th.indeed.com/jobs?{urlencode(params)}'\n",
    "        _ = await page.goto(url)\n",
    "        await page.wait_for_selector('div#mosaic-jobResults')\n",
    "        selector = await page.query_selector('body')\n",
    "        html = await selector.inner_html()\n",
    "    \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        for job_card in soup.find_all('div', class_='cardOutline'):\n",
    "            title = job_card.find('h2', class_='jobTitle').get_text(strip=True)\n",
    "            company = job_card.find('span', {\"data-testid\" : \"company-name\"}).get_text(strip=True)\n",
    "            location = job_card.find('div', {\"data-testid\" : \"text-location\"}).get_text(strip=True) if job_card.find('div', {\"data-testid\" : \"text-location\"}) else 'N/A'\n",
    "            # summary = job_card.find('div', class_='summary').get_text(strip=True)\n",
    "            job_listings.append({'Job Title': title, 'Company': company, 'Location': location})\n",
    "    \n",
    "    await browser.close()\n",
    "    await pw.stop()\n",
    "    # print(job_listings)\n",
    "    return job_listings\n",
    "\n",
    "jobs = await scrape_indeed_jobs('software engineer', 'Pathum Thani')\n",
    "pd.DataFrame(jobs).to_csv('./dataset/indeed_job_listings.csv', index=False)\n",
    "df=pd.read_csv('./dataset/indeed_job_listings.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d89efb-b511-4504-acce-4a3ccc1cd6a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 4: Advanced File Operations\n",
    "**Task**: Write a Python script to scan a directory containing log files (text files). The script should aggregate error messages from all files, count their occurrences, and output a summary in a new text file. Assume a specific pattern in the log files denotes errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5560776-9f79-4549-8e4a-e9846c53b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data source https://github.com/logpai/loghub/blob/master/Linux/Linux_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b4ad7db-9dab-4c9e-b676-3a9f32eda40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1546 entries, 0 to 1545\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   datetime  1546 non-null   object\n",
      " 1   severity  1546 non-null   object\n",
      " 2   message   1546 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 36.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>severity</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jun 14 15:16:01</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[19939]: authentication fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jun 14 15:16:02</td>\n",
       "      <td>WARNING</td>\n",
       "      <td>combo sshd(pam_unix)[19937]: check pass; user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jun 14 15:16:02</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[19937]: authentication fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jun 15 02:04:59</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[20882]: authentication fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jun 15 02:04:59</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[20884]: authentication fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          datetime severity                                            message\n",
       "0  Jun 14 15:16:01    ERROR  combo sshd(pam_unix)[19939]: authentication fa...\n",
       "1  Jun 14 15:16:02  WARNING  combo sshd(pam_unix)[19937]: check pass; user ...\n",
       "2  Jun 14 15:16:02    ERROR  combo sshd(pam_unix)[19937]: authentication fa...\n",
       "3  Jun 15 02:04:59    ERROR  combo sshd(pam_unix)[20882]: authentication fa...\n",
       "4  Jun 15 02:04:59    ERROR  combo sshd(pam_unix)[20884]: authentication fa..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_linux_log(file_path):\n",
    "    log_data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Regex pattern to extract datetime and the entire message\n",
    "            match = re.match(r'(\\w{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2}) (.*)', line)\n",
    "            if match:\n",
    "                datetime, message = match.groups()\n",
    "                severity = \"ERROR\" if \"failure\" in message else \"WARNING\"  # Assuming 'failure' indicates an error\n",
    "                log_data.append({'datetime': datetime, 'severity': severity, 'message': message})\n",
    "\n",
    "    return pd.DataFrame(log_data)\n",
    "\n",
    "# Example usage\n",
    "log_file_path = './dataset/Linux_2k.log'  # Replace with the actual path\n",
    "df = parse_linux_log(log_file_path)\n",
    "df.to_csv('./dataset/structured_log_data.csv', index=False)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0bf64-a789-4394-b7ec-59e41b890751",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 5: Web Scraping Dynamic Content\n",
    "**Task**: Use Python with Selenium to scrape the latest tech news articles from [TechCrunch](https://techcrunch.com/). The script should navigate the site, handle dynamic content loading, and extract the article titles, authors, and publication dates, saving them in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a45b7ef-5d4b-434e-9029-8bb5c5545fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Title             20 non-null     object\n",
      " 1   Author            20 non-null     object\n",
      " 2   Publication Date  20 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 608.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What startup founders need to know about AI he...</td>\n",
       "      <td>Alex Wilhelm</td>\n",
       "      <td>5:00 PM GMT+7•November 26, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sam Altman returns to OpenAI, Apple adopts RCS...</td>\n",
       "      <td>Kyle Wiggers</td>\n",
       "      <td>4:16 AM GMT+7•November 26, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black Friday online buying hits a record $9.8B...</td>\n",
       "      <td>Ingrid Lunden</td>\n",
       "      <td>11:36 PM GMT+7•November 25, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neuralink, Elon Musk’s brain implant startup, ...</td>\n",
       "      <td>Kyle Wiggers</td>\n",
       "      <td>11:25 PM GMT+7•November 25, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fate of US venture capital in China teeters on...</td>\n",
       "      <td>Rita Liao</td>\n",
       "      <td>4:32 PM GMT+7•November 25, 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title         Author  \\\n",
       "0  What startup founders need to know about AI he...   Alex Wilhelm   \n",
       "1  Sam Altman returns to OpenAI, Apple adopts RCS...   Kyle Wiggers   \n",
       "2  Black Friday online buying hits a record $9.8B...  Ingrid Lunden   \n",
       "3  Neuralink, Elon Musk’s brain implant startup, ...   Kyle Wiggers   \n",
       "4  Fate of US venture capital in China teeters on...      Rita Liao   \n",
       "\n",
       "                   Publication Date  \n",
       "0   5:00 PM GMT+7•November 26, 2023  \n",
       "1   4:16 AM GMT+7•November 26, 2023  \n",
       "2  11:36 PM GMT+7•November 25, 2023  \n",
       "3  11:25 PM GMT+7•November 25, 2023  \n",
       "4   4:32 PM GMT+7•November 25, 2023  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "pw = await async_playwright().start()\n",
    "browser = await pw.chromium.launch(headless = False)\n",
    "page = await browser.new_page()\n",
    "url=f'https://techcrunch.com/'\n",
    "_ = await page.goto(url)\n",
    "await page.wait_for_selector('div.content')\n",
    "\n",
    "selector = await page.query_selector('body')\n",
    "html = await selector.inner_html()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "articles =  soup.find_all('article', class_=\"post-block\")\n",
    "\n",
    "data = []\n",
    "for article in articles:\n",
    "    # if not article!=article: break\n",
    "    title =  article.find('h2').get_text(strip=True) \n",
    "    author =  article.find('span', class_ = 'river-byline__authors').get_text(strip=True)\n",
    "    datetime =  article.find('time', class_ = 'river-byline__full-date-time').get_text(strip=True)\n",
    "    data.append({'Title': title, 'Author': author, 'Publication Date': datetime})\n",
    "\n",
    "await browser.close()\n",
    "await pw.stop()\n",
    "df=pd.DataFrame(data)\n",
    "df.to_csv('./dataset/techcrunch.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86608e-9f99-45db-9e98-4b9c7de5ccb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 6: Extracting and Analyzing Data from API\n",
    "**Task**: Write a Python script to fetch weather data from the [OpenWeatherMap API](https://openweathermap.org/api). Extract temperature, humidity, and weather conditions for a specified city, and write this data to a JSON file. Include error handling for invalid city names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457d65d-468e-4a62-ad9f-9ac6c0e83c2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 7: Scraping and Processing E-commerce Product Data\n",
    "**Task**: Create a Python script to scrape product details from an e-commerce site like [Amazon](https://www.amazon.com). Focus on a specific category (e.g., books, electronics). Extract product names, prices, and ratings, and save them in a pandas DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f86d6e-a91a-4963-9e4c-4efe6473b9b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 8: Automated Data Cleaning from a Text File\n",
    "**Task**: Write a Python script to read a text file from [Project Gutenberg](https://www.gutenberg.org/). The script should remove all the headers and footers added by Project Gutenberg, count the frequency of each word in the text, and output the top 10 most frequent words to a new file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c3bdb-ce60-4954-a5dd-d1456c6ee0e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 9: Parsing and Summarizing Data from a News API\n",
    "**Task**: Use the [News API](https://newsapi.org/) to fetch recent news articles on a specific topic (e.g., \"climate change\"). Write a Python script to parse this data, extracting the article title, source, and publication date, and then summarize this data in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4d150-92c6-46a1-bca4-75f601fd1f83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 10: Web Scraping with JavaScript-Rendered Content\n",
    "**Task**: Write a Python script using Selenium to scrape movie ratings and reviews from a site like [Rotten Tomatoes](https://www.rottentomatoes.com/). The script should navigate through a list of movies, handle the dynamically loaded content, and extract the movie title, rating, and a sample of user reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b3f79-7ece-496c-9d7d-a36946dc7388",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8efb84-a82a-4d91-a833-26d8d91c0480",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
